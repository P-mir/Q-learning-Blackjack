The $\epsilon$-greedy strategy did not improve the performance of the agent, and we decided to set $\epsilon=0$. This surprising result can be explained by taking into account that $\gamma=1$ imply that the estimated value of each action-state is greatly dependant of the next action-states values. In other words even if our agent did not explore all the state space, the values of each action-state were correctly estimated thanks to the next action-states