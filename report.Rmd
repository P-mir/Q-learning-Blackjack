---
title: "report"
author: "Patrick Guerin"
date: "6 avril 2018"
output: pdf_document
geometry: top=2cm,bottom=2cm
fontsize: 11pt

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

path="C:/Users/p/Documents/GitHub/Q-learning-Blackjack/images"
 # setwd("C:/Users/p/Documents/GitHub/Q-learning-Blackjack/images")


```
\graphicspath{ {path/} }

```{r, echo=FALSE}

```

#Abstract

goal context

exploring the performance limits of Q-learning

identify is card counting can help

#Context

## The Blackjack game


Gmae description

one deck of 52 card



##Card counting

Ideally our player should know not only the score of the hands but also the actual cards drawn.

one deck:54 cards
approximate number of states: $54^{9}$

need of strategies!
*the exponential number of states*

*Hi-Lo Strategy*


*Number of decks*


*Frequency of deck shuffling*




# The Q-learning algorithm

can be modeled as a markov chain

TD methods can learn directly from raw experience without a model of the environment's dynamics. Like DP, TD methods update estimates based in part on other learned estimates, without waiting for a final outcome

Since We believe the Q-learning approach is particularly adapted to deal with this problem


The game of blackjack can be seen as a Markov decision process, for which each action only depends of the previous one. In order to find an optimal policy, We use the **Q-learning** method. Q-learning can be classified as an off-policy Temporal Difference algorithm.

TD learning is a combination of Monte Carlo ideas and dynamic programming (DP) ideas. Like Monte Carlo methods, TD methods can learn directly from raw experience without a model of the environment. Like DP, TD methods update estimates based in part on other learned estimates, without waiting for a final outcome,contrary with Monte Carlo method which only update at the end of the epochs.

Firstly, we define the Q-table, which links each pair of action-state to its (approximated) expected value . On the basis of this the agent will choose the action with the highest expected value given the current state.

The Q-table is first initialized at zero for all action-states, and we use Temporal Difference error based updates to fill it.

The update is made according to the following formula:

$Q(s_{t},a_{t}) \leftarrow   \underbrace{Q(s_{t},a_{t})}_{\rm old~value} + \underbrace{\alpha}_{\rm learning~rate} \cdot  \overbrace{\bigg( \underbrace{r_{t}}_{\rm reward} + \underbrace{\gamma}_{\rm discount~factor} \cdot \Big[\underbrace{Q(s_{t},a_{t})}_{\rm old~value}- \underbrace{\max_{a}Q(s_{t+1}, a)\Big]}_{\rm estimate~of~optimal~future~value} \bigg) }^{\rm learned~value}$



***Learning rate($\alpha$)***

The learning rate or step size determines to what extent newly acquired information overrides old information

Various condition have been defined in the literature to ensure a convergence[-@convergence], in practice, a learning rate of 0.1 is often used, this is what we use.

***Discount factor ($\gamma$)***


It is the weight given to the future rewards,we set $\gamma$ to 1 since there is no interest to not take into account all futures states in the case of the blackjack game. Hence we set $\gamma=1$ (which gave us the best results).


***Exploration factor ($\epsilon$)***

The Exploration factor is part of $\epsilon$-greedy strategy generally used to garantee that the agent explore all the state space a convenient number of times. The $\epsilon$-greedy strategy did not improve the performance of the agent, and we decided to set $\epsilon=0$. This surprising result can be explained by taking into account that $\gamma=1$ imply that the estimated value of each action-state is greatly dependant of the next action-states values. In other words even if our agent did not explore all the state space, the values of each action-state were correctly estimated thanks to the next action-states

not convincing.. bug? see next state after terminal state why draw has not negative values in 20

#Implementation

The blackjack game is an application particularly interesting since we can use our own experience to model the environment to be as close as possible of the experience of a real player.

We have implemented the environment as separately as possible from the algorithms to mimics the reality. The environment is implemented in file `\text{Environment.R} and the algorithms in the file \text{Policies.R}.

## Benchmarks

In order to assess the performance of our agent, we have implemented a random strategy and a cautious strategy to serve as benchmarks to evaluate our performance.

In the random strategy, the agent take a random action (Draw or Stop) at each step.

In the cautious strategy, the agent choose to draw cards until he reaches 11 or more, then stop the drawing. With this strategy the player never have a hand exceeding 21.

\begin{figure}[h]
\includegraphics{table_C.png}
\centering
\end{figure}

##Game Simulation

functiong with all options depending on card counting or not


##States

insert table of states

##Actions

##Rewards


##Hi-Lo strategy

We created 2 new script to implement the card counting environment and algorithms: \text{Environment_count.R} and \text{Policies_count.R}.


###Number of decks

As the number of decks grows, the number of values that the counter can take also grows and with that the dimension of the action-state space to take into account all possible values of the counter. To keep our Q-table as small as possible, we automatically scale the Q-table in function of the number of decks chosen (up to 10 decks).

### Shuffling of the deck



...



## Statistics









#Issues encountered

environment

reduce as much as possible the number of states 
at the beginning count bt -12 12  25*19*10 states
now 7

ace

how to differenciate as as 3 and as 2 2?

on a préferer ne pas intégrer l ace pour garder une modélisation fidèle a l'environnement énoncé et se concentrer sur reinforcement and state



#Conclusion

The Universal Approximation Theorem shows that neural nets should be able to eventually
adequately represent any Q function we want with a sufficiently large network, we
8
would like to make our larger to see if that could improve any of the issues we are seeing,
especially with our DQN.

 tuning the learning rate hyper-parameter of the optimization algorithm
for both methods, but we would like to do more work and a proper linear search over
the space for ?? and look into some more interesting annealing strategies.



Most cardcounting
strategies provide recommendations about when to up the bet and when to lower it.
In fact, the majority of the card-counting EV comes from this large (up to 100x) modulation
in bet size.


Blackjack in Las Vegas can use one to eight decks


\section{References}

nocite: | 
  @Sutton

@convergence
@Sutton
<!-- typical strategies played by humans -->
<!-- <!-- https://pdfs.semanticscholar.org/e1dd/06616e2d18179da7a3643cb3faab95222c8b.pdf --> -->


