table(Q) #table of learned policy
}
else if(method =="C"){
table_C()
}
}
#to see what's happening use "loud":          game(1000,"loud")
# to simulate a drunk player (Random choice): game(1000,method="R")
game(10000,method="C")
debugSource('C:/Users/p/Documents/GitHub/Q-learning-Blackjack/Q-learning.R', echo=TRUE)
debugSource('C:/Users/p/Documents/GitHub/Q-learning-Blackjack/Q-learning.R', echo=TRUE)
debugSource('C:/Users/p/Documents/GitHub/Q-learning-Blackjack/Q-learning.R', echo=TRUE)
gamma=0.90  #discount
#to see what's happening use "loud":          game(1000,"loud")
# to simulate a drunk player (Random choice): game(1000,method="R")
# To benchmark with a carefull strategy:      game(10000,method="C")
game(1000,method="Q")
#to see what's happening use "loud":          game(1000,"loud")
# to simulate a drunk player (Random choice): game(1000,method="R")
# To benchmark with a carefull strategy:      game(10000,method="C")
game(100000,method="Q")
gamma=1.2  #discount
#to see what's happening use "loud":          game(1000,"loud")
# to simulate a drunk player (Random choice): game(1000,method="R")
# To benchmark with a carefull strategy:      game(10000,method="C")
game(100000,method="Q")
#to see what's happening use "loud":          game(1000,"loud")
# to simulate a drunk player (Random choice): game(1000,method="R")
# To benchmark with a carefull strategy:      game(10000,method="C")
game(100000,method="Q")
gamma=0.5  #discount
#to see what's happening use "loud":          game(1000,"loud")
# to simulate a drunk player (Random choice): game(1000,method="R")
# To benchmark with a carefull strategy:      game(10000,method="C")
game(100000,method="Q")
#to see what's happening use "loud":          game(1000,"loud")
# to simulate a drunk player (Random choice): game(1000,method="R")
# To benchmark with a carefull strategy:      game(10000,method="C")
game(100000,method="Q")
gamma=0.2  #discount
#to see what's happening use "loud":          game(1000,"loud")
# to simulate a drunk player (Random choice): game(1000,method="R")
# To benchmark with a carefull strategy:      game(10000,method="C")
game(100000,method="Q")
gamma=2  #discount
#to see what's happening use "loud":          game(1000,"loud")
# to simulate a drunk player (Random choice): game(1000,method="R")
# To benchmark with a carefull strategy:      game(10000,method="C")
game(100000,method="Q")
gamma=1.4  #discount
#to see what's happening use "loud":          game(1000,"loud")
# to simulate a drunk player (Random choice): game(1000,method="R")
# To benchmark with a carefull strategy:      game(10000,method="C")
game(100000,method="Q")
gamma=1.2  #discount
#to see what's happening use "loud":          game(1000,"loud")
# to simulate a drunk player (Random choice): game(1000,method="R")
# To benchmark with a carefull strategy:      game(10000,method="C")
game(100000,method="Q")
#to see what's happening use "loud":          game(1000,"loud")
# to simulate a drunk player (Random choice): game(1000,method="R")
# To benchmark with a carefull strategy:      game(10000,method="C")
game(100000,method="Q")
gamma=1  #discount
#to see what's happening use "loud":          game(1000,"loud")
# to simulate a drunk player (Random choice): game(1000,method="R")
# To benchmark with a carefull strategy:      game(10000,method="C")
game(100000,method="Q")
epsilon = 0.2   #Exploration factor, reduce it after a while
#to see what's happening use "loud":          game(1000,"loud")
# to simulate a drunk player (Random choice): game(1000,method="R")
# To benchmark with a carefull strategy:      game(10000,method="C")
game(100000,method="Q")
game = function(n_episodes,infos = "quiet",method = "Q",res = TRUE){
reset_stat()
# reset_Qmatrix()
for (i in 1:n_episodes){
party(infos, method)
count()
}
if (res == TRUE){
cat(n_win/n_game,"win: ",n_win,"loss: ",n_loss,"game: ",n_game,"draw",n_draw,"\n payoff: ",n_win-n_loss,"\n")
}
if(method =="Q"){
# table(Q) #table of learned policy
}
else if(method =="C"){
table_C()
}
}
#to see what's happening use "loud":          game(1000,"loud")
# to simulate a drunk player (Random choice): game(1000,method="R")
# To benchmark with a carefull strategy:      game(10000,method="C")
game(300000,method="Q")
epsilon = 0   #Exploration factor, reduce it after a while
#to see what's happening use "loud":          game(1000,"loud")
# to simulate a drunk player (Random choice): game(1000,method="R")
# To benchmark with a carefull strategy:      game(10000,method="C")
game(200000,method="Q")
epsilon = 0.4   #Exploration factor, reduce it after a while
game = function(n_episodes,infos = "quiet",method = "Q",res = TRUE){
reset_stat()
reset_Qmatrix()
for (i in 1:n_episodes){
party(infos, method)
count()
}
if (res == TRUE){
cat(n_win/n_game,"win: ",n_win,"loss: ",n_loss,"game: ",n_game,"draw",n_draw,"\n payoff: ",n_win-n_loss,"\n")
}
if(method =="Q"){
# table(Q) #table of learned policy
}
else if(method =="C"){
table_C()
}
}
#to see what's happening use "loud":          game(1000,"loud")
# to simulate a drunk player (Random choice): game(1000,method="R")
# To benchmark with a carefull strategy:      game(10000,method="C")
game(100000,method="Q")
epsilon = 0   #Exploration factor, reduce it after a while
game = function(n_episodes,infos = "quiet",method = "Q",res = TRUE){
reset_stat()
#reset_Qmatrix()
for (i in 1:n_episodes){
party(infos, method)
count()
}
if (res == TRUE){
cat(n_win/n_game,"win: ",n_win,"loss: ",n_loss,"game: ",n_game,"draw",n_draw,"\n payoff: ",n_win-n_loss,"\n")
}
if(method =="Q"){
# table(Q) #table of learned policy
}
else if(method =="C"){
table_C()
}
}
#to see what's happening use "loud":          game(1000,"loud")
# to simulate a drunk player (Random choice): game(1000,method="R")
# To benchmark with a carefull strategy:      game(10000,method="C")
game(100000,method="Q")
#epsilon=1/num_episode # test to decrease linearly based to num_episode
reset_Qmatrix()
epsilon = 0.3   #Exploration factor, reduce it after a while
#to see what's happening use "loud":          game(1000,"loud")
# to simulate a drunk player (Random choice): game(1000,method="R")
# To benchmark with a carefull strategy:      game(10000,method="C")
game(100000,method="Q")
#to see what's happening use "loud":          game(1000,"loud")
# to simulate a drunk player (Random choice): game(1000,method="R")
# To benchmark with a carefull strategy:      game(10000,method="C")
game(100000,method="Q")
#to see what's happening use "loud":          game(1000,"loud")
# to simulate a drunk player (Random choice): game(1000,method="R")
# To benchmark with a carefull strategy:      game(10000,method="C")
game(100000,method="Q")
#to see what's happening use "loud":          game(1000,"loud")
# to simulate a drunk player (Random choice): game(1000,method="R")
# To benchmark with a carefull strategy:      game(10000,method="C")
game(100000,method="Q")
#to see what's happening use "loud":          game(1000,"loud")
# to simulate a drunk player (Random choice): game(1000,method="R")
# To benchmark with a carefull strategy:      game(10000,method="C")
game(100000,method="Q")
#to see what's happening use "loud":          game(1000,"loud")
# to simulate a drunk player (Random choice): game(1000,method="R")
# To benchmark with a carefull strategy:      game(10000,method="C")
game(100000,method="Q")
#to see what's happening use "loud":          game(1000,"loud")
# to simulate a drunk player (Random choice): game(1000,method="R")
# To benchmark with a carefull strategy:      game(10000,method="C")
game(300000,method="Q")
epsilon = 0   #Exploration factor, reduce it after a while
#to see what's happening use "loud":          game(1000,"loud")
# to simulate a drunk player (Random choice): game(1000,method="R")
# To benchmark with a carefull strategy:      game(10000,method="C")
game(100000,method="Q")
#to see what's happening use "loud":          game(1000,"loud")
# to simulate a drunk player (Random choice): game(1000,method="R")
# To benchmark with a carefull strategy:      game(10000,method="C")
game(100000,method="C")
#to see what's happening use "loud":          game(1000,"loud")
# to simulate a drunk player (Random choice): game(1000,method="R")
# To benchmark with a carefull strategy:      game(10000,method="C")
game(10000,method="C")
debugSource('C:/Users/p/Documents/GitHub/Q-learning-Blackjack/Q-learning.R', echo=TRUE)
debugSource('C:/Users/p/Documents/GitHub/Q-learning-Blackjack/Q-learning.R', echo=TRUE)
debugSource('C:/Users/p/Documents/GitHub/Q-learning-Blackjack/Q-learning.R', echo=TRUE)
s
1
debugSource('C:/Users/p/Documents/GitHub/Q-learning-Blackjack/Q-learning.R', echo=TRUE)
#Decaying epsilon-greedy Q-learning implementation
alpha=0.1
gamma=1  #discount
epsilon = 0   #Exploration factor, reduce it after a while
#epsilon=1/num_episode # test to decrease linearly based to num_episode
reset_Qmatrix()
player_states= seq(2,20)
dealer_states = seq(1,10)
#generate all possible state (state space is 190)
grid=expand.grid(player_states,dealer_states)
S=paste(grid$Var1,grid$Var2,sep="-")
# Set of actions, draw another cards or stick with the cards
A=c("D","S")
# reinitialize the Action-State matrix
reset_Qmatrix = function(){
Q <<- matrix(0,nrow=190,ncol=2)
colnames(Q) <<- A
rownames(Q) <<- S
}
#epsilon-greedy strategy
#return the action that maximize the action-value function for a given state
#if both actions output the same value the agent choose randomly
# in epsilon% of the cases the agent choose randomly anyway (exploration)
# random strategy, benchmark
random_action = function(){
return(sample(A,1))
}
#draw until sum of card is 11 then stop
draw = c()
for (i in seq(1,172,19)){
draw=c(draw,seq(i,i+9))
}
cautious_strategy= function(state){
if (state %in% draw){
return("D")
}
else{
return("S")
}
}
table_C = function(){
Table = matrix(nrow = 19,ncol = 10)
colnames(Table) = seq(1,10)
rownames(Table) = seq(2,20)
for (i in 1:10){
for (j in 1:10){
Table[i,j] = "D"
}
}
for (i in 11:19){
for (j in 1:10){
Table[i,j] = "S"
}
}
return(kable(Table))
}
# epsilon-greedy strategy of the Q-learning method
choose_action = function(state){
if (runif(1)<epsilon){
return(sample(A,1))
}
else{
if (max(Q[state,])==min(Q[state,])){
return(sample(A,1))
}
else{
return(colnames(Q)[which.max(Q[state,])])
}
}
}
# Q-learning update
Qlearning = function(){
# cat("action is: ",action,"\n")
Q[state,action] <<- Q[state,action] + alpha*(reward+gamma*max(Q[state1,])-Q[state,action])
}
#result table, policy learned for each state
table = function(Q){
Table = matrix(nrow = 19,ncol = 10)
colnames(Table) = seq(1,10)
rownames(Table) = seq(2,20)
k=0
for (j in 1:10){
for (i in 1:19){
Table[i,j] = colnames(Q)[which.max(Q[i+k,])]
}
k= k+19
}
return(kable(Table))
}
# Variance depending of the number of iteration
# var_plot= function(){
#   x=c(seq(1,99),seq(100,990,10),seq(1000,9500,500),seq(10000,12000,2000))
#   y=c()
#   k=1
#   for (i in x){
#     game(i,res = FALSE)
#     y[k]=n_win/n_game
#     k = k+1
#   }
#   dt = data.frame(x = x, y = y)
#   ggplot(dt, aes(x , y))+
#     geom_point(color="blue")+
#     ggtitle("")
# }
# var_plot()
# Learning curve, percentage of win according to number of iterations
# average performance on 30 games by default
average_win= function(sample = 30){
x=c(seq(20,30),seq(100,990,10))#,seq(1000,9500,500),seq(10000,15000,1000))
y=c()
perf= c()
k=1
for (i in x){
for(j in 1:sample){
game(i,res = FALSE)
perf[j] = n_win/n_game
}
y[k] = mean(perf)
k = k+1
print(i)
}
# plot(x,y)  #ggplot curve
dt = data.frame(x = x, y = y)
ggplot(dt, aes(x , y))+
geom_line(color="darkblue")+
geom_point(color="darkblue")+
ggtitle("Learning curve")
}
# average_win()
# install.packages("ggplot2")
# install.packages("knitr")
library(ggplot2)
library(knitr)
# 1 = Ace, 2-10 = Number cards, Jack/Queen/King = 10
deck = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 10, 10)
end=FALSE
draw_card = function(deck){
return(sample(deck,1))
}
draw_hand = function(){
return(c(draw_card(deck),draw_card(deck)))
}
# usable ace function
# usable_ace(hand){
#   if (1 %in% hand){
#
#   }
# }
# total of the hand  ,   add condition for usable ace
sum_hand = function(hand){
return(sum(hand))
}
#Is the hand higher than 21?
is_bust = function(hand){
if (sum_hand(hand)>21){
bust = TRUE
}
else {bust=FALSE}
return(bust)
}
#Score of the hand (0 if bust)
score = function(hand){
if (is_bust(hand)){
return(0)
}
else {
return(sum_hand(hand))
}
}
step = function(infos = "quiet",method = "Q"){
state <<- row_Qmatrix()
if (infos == "loud"){
cat("old state: ",state,"\n")
}
if (infos == "loud"){
cat("cards:",p_hand,"\n")
}
if (sum_hand(p_hand)==21){
end <<- TRUE
reward <<- 1
}
else{
if (method == "Q"){
action <<- choose_action(state)
}
else if (method =="R"){
action <<- random_action()
}
else if (method =="C"){
action <<- cautious_strategy(state)
}
cautious_strategy(state)
if (action=="D"){
p_hand <<- append(p_hand,draw_card(deck))
if (infos == "loud"){
cat("draw a card, hand is",p_hand)
}
if (is_bust(p_hand)){
end <<- TRUE
reward <<- -1
if (infos == "loud"){
print("bust")
}
}
else if (sum_hand(p_hand)==21){
end <<- TRUE
reward <<- 1
if (infos == "loud"){
print("blackjack")
}
}
else{
end <<- FALSE
reward <<- 0
if (infos == "loud"){
print('continue')
}
}
}
else{
if (infos == "loud"){
print('stop')
}
end <<- TRUE
while (sum_hand(d_hand) < 17){
d_hand <<- append(d_hand,draw_card(deck))
if (infos == "loud"){
cat("dealer add a cart, hand is",d_hand,"\n")
}
}
if (score(p_hand)>score(d_hand)){
reward <<- 1
if (infos == "loud"){
print("win \n")
}
}
else if (score(p_hand)<score(d_hand)){
reward <<- -1
if (infos == "loud"){
print("loose \n")
}
}
else {reward <<- 0}
}
}
if (infos == "loud"){
cat(reward,score(p_hand),score(d_hand),end,"\n")
}
state1 <<- row_Qmatrix()
if (infos == "loud"){
cat("new state: ",state1,"\n\n")
}
}
#Count number of win,loss,draw and games
count = function(){
if (reward == 1){
n_win <<- n_win+1
} else if (reward == -1){
n_loss <<- n_loss+1
} else{
n_draw <<- n_draw+1
}
n_game <<- n_game+1
}
# Initialize player and dealer hands
reset = function(){
p_hand<<-draw_hand()
d_hand<<-draw_hand()
}
reset_stat = function(){
n_game <<- 0
n_win <<- 0
n_loss <<- 0
n_draw <<- 0
}
#give the index of the Q table row corresponding to current state
row_Qmatrix = function(){
if( score(p_hand) == 0){
}
else if (score(p_hand) == 21){
}
else{
state <<- score(p_hand) + 19 * d_hand[1] - 20
}
return(state)
}
# --------GAME SIMULATION----------
# Run one game of blackjack, can print index of the states in the Q-table,before and after the game
party = function(infos = "quiet",method = "Q"){
reset()
end <<- FALSE
while(end == FALSE){
step(infos, method)
Qlearning()
}
}
game = function(n_episodes,infos = "quiet",method = "Q",res = TRUE){
reset_stat()
#reset_Qmatrix()
for (i in 1:n_episodes){
party(infos, method)
count()
}
if (res == TRUE){
cat(n_win/n_game,"win: ",n_win,"loss: ",n_loss,"game: ",n_game,"draw",n_draw,"\n payoff: ",n_win-n_loss,"\n")
}
if(method =="Q"){
# table(Q) #table of learned policy
}
else if(method =="C"){
table_C()
}
}
#to see what's happening use "loud":          game(1000,"loud")
# to simulate a drunk player (Random choice): game(1000,method="R")
# To benchmark with a carefull strategy:      game(10000,method="C")
game(10000,method="C")
setwd("C:/Users/p/Documents/GitHub/Q-learning-Blackjack/images")
setwd("C:/Users/p/Documents/GitHub/Q-learning-Blackjack/images")
